{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline \n",
    "from mnist import load_mnist\n",
    "train_data, train_labels = load_mnist(dataset='training', path='./')\n",
    "test_data, test_labels = load_mnist(dataset='testing', path='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = np.reshape(train_data, (60000, 28 * 28)).T\n",
    "test_data  = np.reshape(test_data,  (10000, 28 * 28)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (60000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINBIN=np.where(train_data < 0.5, 0, 1);\n",
    "TESTBIN=np.where(test_data < 0.5, 0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1 : Théorie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $K$ le nombre de distribution de Bernouilli.<br />\n",
    "Soit $D$ le nombre de variables.<br />\n",
    "Soit $N$ le nombre de données.<br />\n",
    "Soit $X$ une matrice de taille $D*N$ contenant nos données d'apprentissage.<br />\n",
    "Soit $\\mu$ une matrice de taille $D*K$ contenant les moyennes de chaque distribution de Bernouilli.<br />\n",
    "Soit $\\Pi$ un vecteur de taille $K$ contenant le poid de chaque distribution.<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape E:\n",
    "L'tape E revient à calculer les probabilités des composante, sachant le dataset. Cette probabilité sera ensuite utilisé pour éstimer les nouveaux parametres qui maximise la vraiessemblance. En utilisant la formule de Bayes, on obtient:\n",
    "\n",
    "$$P(\\mu^{(k)}, \\Pi^{(k)} | X^{(j)}) = \n",
    "\\frac{P(X^{(j)}|\\mu^{(k)})\\Pi^{(k)}}{\\sum_{k=1}^K{ P(X^{(j)}|\\mu^{(k)})\\Pi^{(k)}}}$$\n",
    "\n",
    "S'il y a K composantes et N images, il faut donc calculer $N * K$ probabilité \n",
    "En supposant que les images suivent une loi de bernouilli, on a:\n",
    "$$P(X^{(j)}|\\mu^{(k)}) = \\prod_{i=1}^{D}{(\\mu^{(k)}_{i})^{X^{(j)}_{i}}(1 - \\mu^{(k)}_{i})^{(1 - X^{(j)}_{i})}}$$\n",
    "\n",
    "En passant au log, on obtient:\n",
    "\n",
    "$$log P(X^{(j)}|\\mu^{(k)}) = {\\sum_{i=1}^D{{X^{(j)}_{i}}log({\\mu^{(k)}_{i}}) + {(1 - X^{(j)}_{i})}log(1 - \\mu^{(k)}_{i})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape M\n",
    "\n",
    "L'etape M revient à calculer les parametres qui maximisent le log de vraisemblance. Dans une distribution de Bernouilli, on obtient après calcule:\n",
    "\n",
    "$$\\Pi^{(k)} = \\frac{\\sum_{j=1}^N P(\\mu^{(k)}, \\Pi^{(k)} | X^{(j)})}{N}$$\n",
    "\n",
    "$$\\mu^{(k)}_{i} = \\frac{\\sum_{j=1}^N P(\\mu^{(k)}, \\Pi^{(k)} | X^{(j)}) X^{(j)}_{i}}{\\sum_{j=1}^N P(\\mu^{(k)}, \\Pi^{(k)} | X^{(j)})}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 : Implémentation\n",
    "\n",
    "Dans cette partie on implemente la classe EM qui apprend à maximiser le log de vraisemblance\n",
    "\n",
    "Il faut faire attention à la manipulation des valeurs de probabilités (entre 0 et 1\\) qui deviennent rapidement nulles à cause de la précision des chiffres flottants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import matlib \n",
    "\n",
    "class EM:\n",
    "    \n",
    "    def __expectation(self, with_sigma):\n",
    "        \"\"\"Computes log of P(parameters| Xi) for all Xi in the data set X\n",
    "            Expectation step of the EM \n",
    "            \n",
    "            :param with_sigma: a boolean indicating whether to use sigma when computing the probability.    \n",
    "            :rtype: numpy array of size (K, N)\n",
    "        \"\"\"\n",
    "        \n",
    "        w = np.copy(self.weights)\n",
    "        w[w == 0] = 1e-10\n",
    "        ln_weights = np.log(w)\n",
    "        ln_probk = np.empty((self.K, self.N))\n",
    "        for k in range(self.K): \n",
    "            if not with_sigma:\n",
    "                p = self.proba_log_func(self.data, self.means[:, k]) \n",
    "            else:\n",
    "                p = self.proba_log_func(self.data, self.means[:, k], self.sigmas[:, :, k])\n",
    "            ln_probk[k, :] = p + ln_weights[k]\n",
    "        prob = np.sum(np.exp(ln_probk), axis = 0)\n",
    "        prob[prob == 0] = 1e-10\n",
    "        self.log_likelihood = np.sum(np.log(prob)) # saving log-likelihood\n",
    "        ln_probk -= np.log(prob) # num/denum <=> log(num) - log(denum)\n",
    "        return ln_probk\n",
    "        \n",
    "    def __maximization(self, ln_prob, with_sigma):\n",
    "        \"\"\"Computes the new parameters that maximizes the log-likelihood \n",
    "            Maximization step of the EM\n",
    "            \n",
    "            :param ln_prob: the numpy array returned by __expectation\n",
    "            :param with_sigma: a boolean indicating whether to use sigma when computing the new parameters.\n",
    "            :rtype: 3 numpy arrays: numpy array of size (K), \n",
    "                                    numpy array of size (D, K),\n",
    "                                    numpy array of size (D, D, K) \n",
    "        \"\"\"\n",
    "        \n",
    "        prob = np.exp(ln_prob)\n",
    "        Nk = np.sum(prob, axis = 1)\n",
    "        weights = Nk / self.N\n",
    "        Nk[Nk == 0] = 1e-10\n",
    "        means = np.empty((self.D, self.K))\n",
    "        if with_sigma:\n",
    "            sigmas = np.empty((self.D, self.D, self.K))\n",
    "        for k in range(self.K):\n",
    "            means[:, k] = self.maximizing_mean_func(prob[k, :], self.data)\n",
    "            if with_sigma:\n",
    "                sigma = self.maximizing_sigma_func(prob[k, :], self.data, self.means[:, k]) / Nk[k]\n",
    "                while np.linalg.det(sigma) == 0:\n",
    "                    sigma = self.__floor_sigma(sigma)\n",
    "                sigmas[:, :, k] = sigma\n",
    "        if with_sigma:\n",
    "            return weights, means / Nk, sigmas\n",
    "        return weights, means / Nk, None     \n",
    "    \n",
    "    def __floor_sigma(self, sigma, rep = False):\n",
    "        \"\"\"Computes the new sigma floored to 1\n",
    "            \n",
    "            :param sigma: sigma to be floored  \n",
    "            :rtype: numpy array of size (D, D)\n",
    "        \"\"\"\n",
    "        \n",
    "        if rep:\n",
    "            floor = self.floor * 2\n",
    "            L = np.linalg.cholesky(floor)\n",
    "            L_inv = np.linalg.inv(L)\n",
    "        else:\n",
    "            L = self.L\n",
    "            L_inv = self.L_inv\n",
    "        T = L_inv @ sigma @ L_inv.T\n",
    "        eig_values, eig_vectors = np.linalg.eigh(T)\n",
    "        D = np.diag(eig_values)\n",
    "        D[D < 1] = 1\n",
    "        T = eig_vectors @ D @ np.linalg.inv(eig_vectors) \n",
    "        return L @ T @ L.T\n",
    "    \n",
    "    def __init_floor(self):\n",
    "        \"\"\"Initialise the floor of sigma.\n",
    "            \n",
    "            :rtype: None\n",
    "        \"\"\"\n",
    "        \n",
    "        cov = np.cov(self.data)\n",
    "        self.floor = cov * 0.2\n",
    "        self.L = np.linalg.cholesky(self.floor)\n",
    "        self.L_inv = np.linalg.inv(self.L)\n",
    "        \n",
    "            \n",
    "    def set_em_functions(self, proba_log_func, maximizing_mean_func, maximizing_sigma_func = None):\n",
    "        \"\"\"Sets the functions to be used while training the EM\n",
    "            \n",
    "            :param proba_log_func: the function which computes the log of a probability\n",
    "            :param maximizing_mean_func: the function which computes the mean maximizing the log-likelihood\n",
    "            :param maximizing_sigma_func: the function which computes the sigma maximizing the log-likelihood\n",
    "            :rtype: None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.proba_log_func = proba_log_func\n",
    "        self.maximizing_mean_func = maximizing_mean_func\n",
    "        self.maximizing_sigma_func = maximizing_sigma_func\n",
    "    \n",
    "    def init_parameters(self, data, K, weights = None, means = None, sigmas = None):\n",
    "        \"\"\"Initialise the parameters to be used in the EM.\n",
    "            If parameters are not specified, they are computed in the class using:\n",
    "                __init_weights, __init_means and __init_sigmas\n",
    "            \n",
    "            :param data: an numpy array containing the training set.\n",
    "            :param K: an int containing the number of components in the mixture model\n",
    "            :param weights: an numpy array containing the initial weight of each component.\n",
    "            :param means: an numpy array containing the initial mean of each component.\n",
    "            :param sigmas: an numpy array containing the initial sigma of each component.\n",
    "            :rtype: None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.log_likelihood = -float('Inf')\n",
    "        self.data = data\n",
    "        self.K = K\n",
    "        self.D = data.shape[0]\n",
    "        self.N = data.shape[1]\n",
    "        self.weights = weights\n",
    "        self.means = means\n",
    "        self.sigmas = sigmas\n",
    "        if weights is None:\n",
    "            self.__init_weights()\n",
    "        if means is None:\n",
    "            means = self.__init_means()\n",
    "        if self.maximizing_sigma_func is not None:\n",
    "            if sigmas is None:\n",
    "                sigmas = self.__init_sigmas()\n",
    "            self.__init_floor() \n",
    "        \n",
    "    def __init_weights(self):\n",
    "        \"\"\"Initialise the weight of each component.\n",
    "        \n",
    "            :rtype: None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weights = np.ones(self.K) / self.K\n",
    "        \n",
    "    def __init_means(self):\n",
    "        \"\"\"Initialise the mean of each component.\n",
    "            Uses LBG to initialise means\n",
    "            :rtype: None\n",
    "        \"\"\"\n",
    "        \n",
    "        mean = np.mean(self.data, axis = 1)\n",
    "        self.means = np.empty((self.D, self.K))\n",
    "        e = 0.1\n",
    "        for k in range(self.K):\n",
    "            for d in range(self.D):\n",
    "                if random.random() > 0.5:\n",
    "                    self.means[d, k] = min(mean[d] + e, 1)\n",
    "                else:\n",
    "                    self.means[d, k] = max(mean[d] - e, 0)\n",
    "                    \n",
    "    def __init_sigmas(self):\n",
    "        \"\"\"Initialise the sigma of each component.\n",
    "        \n",
    "            :rtype: None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sigmas = np.empty((self.D, self.D, self.K))\n",
    "        for k in range(K):\n",
    "            self.sigmas[:, :, k] = np.eye((self.D))\n",
    "    \n",
    "    def train(self, with_sigma = False, eps = 1000): \n",
    "        \"\"\"Train the EM and returns the parameters which maximizes the log-likelihood.\n",
    "            \n",
    "            :param with_sigma: a boolean indicating whether to use sigma in EM.\n",
    "            :param eps: an int representing the log-likelihood threshold\n",
    "            :rtype: 2 or 3 numpy arrays depending on the value of with_sigma\n",
    "                 with_sigma = False: numpy array of size (K), numpy array of size (D, K),\n",
    "                 with_sigma = True: adds an numpy array of size (D, D, K) to the return value.\n",
    "        \"\"\"\n",
    "        \n",
    "        diff = float('Inf')\n",
    "        i = 0\n",
    "        while (diff > eps) :\n",
    "            last_log_likelihood = self.log_likelihood\n",
    "            ln_prob = self.__expectation(with_sigma)\n",
    "            new_weights, new_means, new_sigmas = self.__maximization(ln_prob, with_sigma)\n",
    "            diff = np.abs(last_log_likelihood - self.log_likelihood)\n",
    "            self.means = new_means\n",
    "            self.weights = new_weights\n",
    "            self.sigmas = new_sigmas\n",
    "        if with_sigma:\n",
    "            return self.weights, self.means, self.sigmas\n",
    "        return self.weights, self.means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bernouilli(X, Mu):\n",
    "    \"\"\"Computes log of P(X | Mu) for all Xi in the data set X using Bernouilli distribution\n",
    "            \n",
    "            :param X: numpy array containg the data set with rows as features\n",
    "            :param Mu: numpy array containg the estimated mean of the dataset X\n",
    "            :rtype: numpy array of size (N)\n",
    "    \"\"\"\n",
    "    \n",
    "    N = X.shape[1]\n",
    "    M = np.matlib.repmat(Mu, N, 1).T\n",
    "    P = M * X + (1 - M) * (1 - X)\n",
    "    P[P == 0] = 1 # to avoid log(0) and not take care of this value cz log(1) = 0\n",
    "    return np.sum(np.log(P), axis = 0)\n",
    "\n",
    "def bernouilli_means(P, X):\n",
    "    \"\"\"Computes the mean maximizing the log-likelihood of the data set X using Bernouilli distribution\n",
    "            \n",
    "            :param P: numpy array containg P(X | parameters)\n",
    "            :param X: numpy array containg the data set with rows as features\n",
    "            :rtype: numpy array of size (D)\n",
    "    \"\"\"\n",
    "    \n",
    "    D = X.shape[0]\n",
    "    p = np.matlib.repmat(P, D, 1)\n",
    "    return np.sum(p * X, axis = 1)\n",
    "\n",
    "def train_bernouilli(K):\n",
    "    \"\"\"Launches the EM on each class of the TRAINBIN with K components using Bernouilli distribution.\n",
    "            \n",
    "            :param K: an int containg the number of components in each class.\n",
    "            :rtype: numpy array of size (K), numpy array of size (D, K)\n",
    "    \"\"\"\n",
    "    \n",
    "    nb_class = 10\n",
    "    weights = [None] * nb_class\n",
    "    means = [None] * nb_class\n",
    "    em = EM()\n",
    "    em.set_em_functions(bernouilli, bernouilli_means)\n",
    "    for i in range(nb_class):\n",
    "        data = TRAINBIN[:, np.where(train_labels == i)[0]]\n",
    "        em.init_parameters(data, K)\n",
    "        w, m = em.train()\n",
    "        weights[i] = w\n",
    "        means[i] = m\n",
    "    return np.array(weights), np.array(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K = 1\n",
    "weights, means = train_bernouilli(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Afficher les moyennes des différentes distributions de Bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAACbCAYAAABVqOFYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11177f320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import visualize as vz\n",
    "tmp = means.T\n",
    "vz.plotGroupImages(tmp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois entrainé, on peut alors tester un classifieur bayésien en utilisant un mélange pour chaque classe. On testera le classifieur avec 1, 2, 4 et 8 composantes par classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayes_class(data, K, weights, means, proba_log_func, sigmas = None):\n",
    "    \"\"\"Naive Bayes Classifier of the data set data\n",
    "            \n",
    "            :param data: numpy array containg the data set with rows as features\n",
    "            :param K: int containg the number of components in the dataset X\n",
    "            :param weights: numpy array containg the weight of each component in the dataset X\n",
    "            :param means: numpy array containg the mean of each component in the dataset X\n",
    "            :param proba_log_func: the function which computes the log of a probability\n",
    "            :param sigmas: numpy array containg the sigma of each component in the dataset X\n",
    "            :rtype: numpy array of size (N)\n",
    "    \"\"\"\n",
    "    D = data.shape[0]\n",
    "    N = data.shape[1]\n",
    "    nb_class = 10\n",
    "    prob = np.zeros((nb_class, N))\n",
    "    ln_weights = np.log(weights)\n",
    "    for c in range(nb_class):\n",
    "        ln_p = np.empty((K, N))\n",
    "        for k in range(K):\n",
    "            if sigmas is None:\n",
    "                ln_p[k, :] = proba_log_func(data, means[c, :, k]) + ln_weights[c, k]    \n",
    "            else:\n",
    "                ln_p[k, :] = proba_log_func(data, means[c, :, k], sigmas[c, :, :, k]) + ln_weights[c, k] \n",
    "        p = np.sum(np.exp(ln_p), axis = 0)\n",
    "        prob[c, :] = p\n",
    "    return np.argmax(prob, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sur 1, 2, 4 et 8 composantes par classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  1 , Precision:  0.8132\n",
      "K:  2 , Precision:  0.8548\n",
      "K:  4 , Precision:  0.8896\n",
      "K:  8 , Precision:  0.9152\n"
     ]
    }
   ],
   "source": [
    "weights = {}\n",
    "means = {}\n",
    "out = {}\n",
    "for K in [1, 2, 4, 8]:\n",
    "    w, m = train_bernouilli(K)\n",
    "    t_data = TESTBIN\n",
    "    o = bayes_class(test_data, K, w, m, bernouilli)\n",
    "    print(\"K: \" ,K , \", Precision: \" ,(o == test_labels).sum() / test_labels.shape[0])\n",
    "    weights[str(K)] = w\n",
    "    means[str(K)] = m\n",
    "    out[str(K)] = o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 3 : Comparaison avec un GMM\n",
    "\n",
    "Dans cette partie, on va comparer les résultats obtenus avec le cas d’un mélange de gaussiennes sur les données brutes TRAIN et TEST avec un mélange de bernouilli.\n",
    "\n",
    "* On commencera par le cas de gaussienne avec des matrices de covariance diagonale, ensuite tester la version avec matrices de covariance complète.\n",
    "* On fera attention à la dégénérescence de l’algorithme EM (matrice de covariance non inversible). Il faut voir comment gérer ce problème ainsi que l’initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "class PCA:\n",
    "    \n",
    "    def __compute_eig(self, data):\n",
    "        cov = np.cov(data)\n",
    "        self.means = np.mean(data, axis = 1)\n",
    "        eig_values, eig_vectors = LA.eig(cov)\n",
    "        if eig_values.imag.sum() != 0:\n",
    "            print(\"Error: covariance of data is not symetrique. \\\n",
    "            It has some complex eigen values.\")\n",
    "        idx = eig_values.argsort()[::-1] \n",
    "        self.eig_values = eig_values[idx].real\n",
    "        self.eig_vectors= eig_vectors[:,idx].real\n",
    "\n",
    "    def __project(self, data, n = -1):\n",
    "        if n == -1:\n",
    "            n = self.get_vectorsNb(0.5)\n",
    "        centeredData = data - np.matlib.repmat(self.means, data.shape[1], 1).T\n",
    "        return self.eig_vectors[:, :n].T @ (centeredData)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_vectorsNb(self, error_percent):\n",
    "        if error_percent < 0 or error_percent > 100:\n",
    "            print(\"error_percent should be between 0 and 100\")\n",
    "            return -1\n",
    "        info_percent = (100 - error_percent) / 100.0\n",
    "        eigSum = np.sum(self.eig_values)\n",
    "        for i in range(self.eig_values.shape[0]):\n",
    "            if np.sum(self.eig_values[:i]) / eigSum >= info_percent :\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "    def extract_features(self, train_data, test_data, n = 50):\n",
    "        if n == 0:\n",
    "            return train_data, test_data\n",
    "        self.__compute_eig(train_data)\n",
    "        return pca.__project(train_data, n), pca.__project(test_data, n)\n",
    "    \n",
    "    def reproject_features(self, features, n = 50):\n",
    "        N = features.shape[1]\n",
    "        return self.eig_vectors[:, :n] @ features + np.matlib.repmat(self.means, N, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# il faut faire une projection pca pour rendre la matrice de variance inversible\n",
    "pca = PCA()\n",
    "train, test = pca.extract_features(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal(X, Mu, Sigma):\n",
    "    \"\"\"Computes log of P(X | Mu, Sigma) for all Xi in the data set X using Normal distribution\n",
    "            \n",
    "            :param X: numpy array containg the data set with rows as features\n",
    "            :param Mu: numpy array containg the estimated sigma of the dataset X\n",
    "            :param Mu: numpy array containg the estimated mean of the dataset X\n",
    "            :rtype: numpy array of size (N)\n",
    "    \"\"\"\n",
    "    \n",
    "    D = X.shape[0]\n",
    "    N = X.shape[1]\n",
    "    det = np.linalg.det(Sigma)\n",
    "    sigma_inv = np.linalg.inv(Sigma)\n",
    "    P = np.full((N), np.log(det) + D * np.log(2 * math.pi))\n",
    "    for i in range(N):\n",
    "        P[i] +=  (X[:, i] - Mu).T @ sigma_inv @ (X[:, i] - Mu)\n",
    "    P *= -0.5 \n",
    "    return P\n",
    "\n",
    "def normal_means(P, X):\n",
    "    \"\"\"Computes the mean maximizing the log-likelihood of the data set X using Normal distribution\n",
    "            \n",
    "            :param P: numpy array containg P(X | parameters)\n",
    "            :param X: numpy array containg the data set with rows as features\n",
    "            :rtype: numpy array of size (D)\n",
    "    \"\"\"\n",
    "    \n",
    "    D = X.shape[0]\n",
    "    p = np.matlib.repmat(P, D, 1)\n",
    "    return np.sum(p * X, axis = 1)\n",
    "\n",
    "def normal_sigmas(P, X, Mu):\n",
    "    \"\"\"Computes the full covariance matrix maximizing the log-likelihood of the data set X using Normal distribution\n",
    "            \n",
    "            :param P: numpy array containg P(X | parameters)\n",
    "            :param X: numpy array containg the data set with rows as features\n",
    "            :param Mu: numpy array containg the estimated mean of the dataset X\n",
    "            :rtype: numpy array of size (D, D)\n",
    "    \"\"\"\n",
    "    \n",
    "    D = X.shape[0]\n",
    "    N = X.shape[1]\n",
    "    sigma = np.zeros((D, D))\n",
    "    for i in range(N):\n",
    "        m = (X[:, i] - Mu).reshape(D, 1)\n",
    "        s = m @ m.T\n",
    "        sigma += P[i] * s\n",
    "    return sigma\n",
    "   \n",
    "def normal_diag_sigmas(P, X, Mu):\n",
    "    \"\"\"Computes the diagonal covariance matrix maximizing the log-likelihood of the data set X \n",
    "       using Normal distribution\n",
    "            \n",
    "            :param P: numpy array containg P(X | parameters)\n",
    "            :param X: numpy array containg the data set with rows as features\n",
    "            :param Mu: numpy array containg the estimated mean of the dataset X\n",
    "            :rtype: numpy array of size (D, D)\n",
    "    \"\"\"\n",
    "    \n",
    "    cov = normal_sigmas(P, X, Mu)\n",
    "    return np.diag(cov.diagonal())\n",
    "\n",
    "def train_gauss_diag(train_data, K):\n",
    "    \"\"\"Launches the EM on each class of the train_data with K components \n",
    "       using Normal distribution with diagonal covariance matrix. (Independant components)\n",
    "            \n",
    "            :param train_data: numpy array containg the data set with rows as features\n",
    "            :param K: an int containg the number of components in each class.\n",
    "            :rtype: numpy array of size (K), numpy array of size (D, K), numpy array of size (D, D, K)\n",
    "    \"\"\"\n",
    "    \n",
    "    nb_class = 10\n",
    "    weights = [None] * nb_class\n",
    "    means = [None] * nb_class\n",
    "    sigmas = [None] * nb_class\n",
    "    em = EM()\n",
    "    em.set_em_functions(normal, normal_means, normal_diag_sigmas)\n",
    "    for i in range(nb_class):\n",
    "        data = train_data[:, np.where(train_labels == i)[0]] \n",
    "        em.init_parameters(data, K)\n",
    "        w, m, s = em.train(with_sigma = True)\n",
    "        weights[i] = w\n",
    "        means[i] = m\n",
    "        sigmas[i] = s\n",
    "    return np.array(weights), np.array(means), np.array(sigmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec une matrice de variance-covariance diagonale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  1 , Precision:  0.8781\n",
      "K:  2 , Precision:  0.8874\n",
      "K:  4 , Precision:  0.908\n",
      "K:  8 , Precision:  0.9203\n"
     ]
    }
   ],
   "source": [
    "weights = {}\n",
    "means = {}\n",
    "sigmas = {}\n",
    "out = {}\n",
    "train, test = extract_features(train_data, test_data)\n",
    "for K in [1, 2, 4, 8]:\n",
    "    w, m, s = train_gauss_diag(train, K)\n",
    "    o = bayes_class(test, K, w, m, normal, s)\n",
    "    print(\"K: \" ,K , \", Precision: \" ,(o == test_labels).sum() / test_labels.shape[0])\n",
    "    weights[str(K)] = w\n",
    "    means[str(K)] = m\n",
    "    sigmas[str(K)] = s\n",
    "    out[str(K)] = o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec une matrice de variance-covariance complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gauss(train_data, K):\n",
    "    \"\"\"Launches the EM on each class of the train_data with K components \n",
    "       using Normal distribution with full covariance matrix. (corrolated components)\n",
    "            \n",
    "            :param train_data: numpy array containg the data set with rows as features\n",
    "            :param K: an int containg the number of components in each class.\n",
    "            :rtype: numpy array of size (K), numpy array of size (D, K), numpy array of size (D, D, K)\n",
    "    \"\"\"\n",
    "    \n",
    "    nb_class = 10\n",
    "    weights = [None] * nb_class\n",
    "    means = [None] * nb_class\n",
    "    sigmas = [None] * nb_class\n",
    "    em = EM()\n",
    "    em.set_em_functions(normal, normal_means, normal_sigmas)\n",
    "    for i in range(nb_class):\n",
    "        data = train_data[:, np.where(train_labels == i)[0]] \n",
    "        em.init_parameters(data, K)\n",
    "        w, m, s = em.train(with_sigma = True)\n",
    "        weights[i] = w\n",
    "        means[i] = m\n",
    "        sigmas[i] = s\n",
    "    return np.array(weights), np.array(means), np.array(sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  1 , Precision:  0.9637\n",
      "K:  2 , Precision:  0.9642\n",
      "K:  4 , Precision:  0.9701\n",
      "K:  8 , Precision:  0.9767\n"
     ]
    }
   ],
   "source": [
    "weights = {}\n",
    "means = {}\n",
    "sigmas = {}\n",
    "out = {}\n",
    "for K in [1, 2, 4, 8]:\n",
    "    w, m, s = train_gauss(train, K)\n",
    "    o = bayes_class(test, K, w, m, normal, s)\n",
    "    print(\"K: \" ,K , \", Precision: \" ,(o == test_labels).sum() / test_labels.shape[0])\n",
    "    weights[str(K)] = w\n",
    "    means[str(K)] = m\n",
    "    sigmas[str(K)] = s\n",
    "    out[str(K)] = o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque facilement que le passage au GMM avec des matrices de variance covariance complète donne une meilleur classification que toutes les autres methodes qu'on a essayé dans ce tp, surtout quand on augmente K. Pour avoir une meuilleur performence, il suffit d'entrener le modèle avec un nombre plus grand de composante par classe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
